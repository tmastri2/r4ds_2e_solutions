---
title: "Ch.16 Solutions"
execute:
  collapse: true
format: html
editor: visual
---

```{r}
#| echo: false
source("block_settings.R")
```

## Prerequisites:

------------------------------------------------------------------------

```{r}
#| message: FALSE
library(tidyverse)
```

## 16.3.1 Exercises:

1.  The two things I didn't like about the default plot is the crowded x-axis labeling and the inclusion of multiple null options. While NAs definitely have importance in any statistical and exploratory work, for a "pretty" graph I prefer to remove them.

    ```{r}
    gss_cat |> 
      ggplot(aes(x = rincome)) + 
      geom_bar()
    ```

    ```{r}
    gss_cat |> 
      filter(str_detect(rincome, '\\$')) |> 
      ggplot(aes(y = rincome)) + 
      geom_bar()
    ```

2.  I did this through the count function. Can use an expression like `filter(n == max(n))` if you only want the single largest count.

    ```{r}
    gss_cat |> 
      count(relig, sort = TRUE)

    gss_cat |> 
      count(partyid, sort = TRUE)
    ```

3.  I did this by finding the number of distinct denominations options that have appeared for each religion.

    ```{r}
    gss_cat |> 
      group_by(relig) |> 
      summarize(distinct_relig = n_distinct(denom)) |> 
      arrange(desc(distinct_relig))
    ```

    ```{r}
    gss_cat |> 
      group_by(relig) |>
      summarize(distinct = n_distinct(denom)) |> 
      ggplot(aes(y = relig, x = distinct)) +
      geom_bar(stat = 'identity')
    ```

    -   Further analysis will show you that only protestant has specific denominations listed, since Christian and Other only have quasi-variations of null.

## 16.4.1 Exercises:

1.  While median is immune to skew and worth comparing to mean to help show said skew, it's important to realize that skew is something not to be eliminated but a characteristic of your data set worth understanding before you perform transformations of the data. Personally, while I think values above 20 tvhours are almost certainly outliers, mean is still a valuable summary statistic to me since it helps me be alert to potential skew (since it is so different than median) before I ever run a test of skewness.

    ```{r}
    gss_cat |> 
      filter(!is.na(tvhours)) |> 
      ggplot(aes(tvhours)) +
      geom_histogram(binwidth = 1)

    print(mean(gss_cat$tvhours, na.rm = TRUE))

    print(median(gss_cat$tvhours, na.rm = TRUE))
    ```

2.  Included my notes after each function call.

    ```{r}
    levels(gss_cat$marital)
    #Marital: Seems pretty arbitrary.

    levels(gss_cat$race)
    #race: seems pretty arbitrary.

    levels(gss_cat$rincome)
    #Income levels are ordered sensibly. I think it's principled.

    levels(gss_cat$partyid)
    #partyid is ordered from right to left wing. principled.

    levels(gss_cat$relig)
    #relig seems pretty abitrary.

    levels(gss_cat$denom)
    #denom seems pretty abitrary.
    ```

3.  Its because having x as numeric and y as the categorical variable is analogous to flipping the coordinates around the origin from a plot with x as the categorical and y as the numeric. Therefore the y-axis is "counting up" as you move up vertically and the first position is the one closes to the origin.

## 16.5.1 Exercises:

1.  I decided to group many factor levels together to simplify my graph. Granted, this is not without issues since you can justifiably categorize a value like "Ind,near dem" as either democrat or independent.

    ```{r}
    gss_cat |> 
      mutate(
        partyid = fct_collapse(partyid, 
          "other" = c("No answer", "Don't know", "Other party"),
          "rep" = c("Strong republican", "Not str republican"),
          "ind" = c("Ind,near rep", "Independent", "Ind,near dem"),
          "dem" = c("Not str democrat", "Strong democrat")
        )
      ) |> 
      group_by(year, partyid) |> 
      summarise(n = n()) |> 
      mutate(prop = n / sum(n)) |> 
      ggplot(aes(year, prop, color = partyid)) +
      geom_smooth(se = FALSE)
    ```

2.  I decided to create \$5000 partitions.

    ```{r}
    gss_cat |> 
      mutate(
        rincome = fct_collapse(rincome, 
          '0-5k' = c('Lt $1000', '$1000 to 2999', '$3000 to 3999', '$4000 to 4999'),
          '5-10k' = c('$5000 to 5999', '$6000 to 6999', '$7000 to 7999', '$8000 to 9999'))) |> distinct(rincome)
    ```

3.  `fct_lump()` will roll uncommon values into Other by default, and since that is already a top 10 value in relig, there is no need to create an 11th option. If you change the name of "Other" you then get 11th factor levels (including Other).

    ```{r}
    gss_cat |> 
      count(relig, sort = TRUE)

    gss_cat |>
      mutate(relig = fct_lump_n(relig, n = 10)) |>
      count(relig, sort = TRUE)

    gss_cat |>
      mutate(relig = fct_recode(relig,
        'other' = 'Other'
      )) |> 
      mutate(relig = fct_lump_n(relig, n = 10)) |>
      count(relig, sort = TRUE)
    ```
